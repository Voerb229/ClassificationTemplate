{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAM PREP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as dists\n",
    "import statsmodels.api as sm\n",
    "### \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import cluster\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "df = pd.read_csv('data.csv', delimiter=\";\")\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "## Export\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "## Path \n",
    "import os\n",
    "print(os.getcwd()) \n",
    "df = pd.read_csv(\"/Users/username/Documents/my_folder/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info of data \n",
    "print(df.info())\n",
    "df.head()\n",
    "p_product = df[\"product\"].value_counts(normalize=True)\n",
    "pd.plotting.scatter_matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns - kann be done for both private and training data \n",
    "## entweder so oder mit df = ...\n",
    "\n",
    "df.rename(columns={\n",
    "    'uid': 'UserId',\n",
    "    'p': 'Premium',\n",
    "    'm1': 'Minutes1',\n",
    "    'm2': 'Minutes2',\n",
    "    'm3': 'Minutes3' \n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP Values - Gleiches Benennen aller values die das gleiche hei0en sollen\n",
    "# achtung wichtig alle in map unterzubrignen auch die sich nicht ändern  --> Checken ob true\n",
    "df['column'] = df['column'].map({'0': False, \n",
    "                                         '1': True,\n",
    "                                         'Yes': True,\n",
    "                                         'No': False},)\n",
    "\n",
    "# wenn nicht alle Values nennen:\n",
    "\n",
    "df['column'] = df['column'].apply(lambda x: {'0': False, '1': True, 'Yes': True, 'No': False}.get(x, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map Category to numerical - als Schleife besser als one hot für unsere modelle \n",
    "unique_x1 = pd[\"x1\"].unique()\n",
    "mapping = {ingr: i for i, ingr in enumerate(unique_x1)}\n",
    "pd[\"x1\"] = pd[\"x1\"].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oder One Hot >Encoding - trotzdem noch astype setzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One Hot Ending - nur für Log Reg\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply one-hot encoding to non-numeric columns -## is needed for logistic regression not for other \n",
    "df = pd.get_dummies(df, columns=non_numeric_cols, drop_first=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation / Einheiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wechselkurs definieren\n",
    "wechselkurs_dollar_euro = 0.85\n",
    "\n",
    "# Zahlen bereinigen und umrechnen\n",
    "df['Einkommen'] = df['Einkommen'].apply(\n",
    "    lambda x: float(x.replace(',', '').replace('€', '')) if '€' in x else\n",
    "              float(x.replace(',', '').replace('$', '')) * wechselkurs_dollar_euro\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## oder \n",
    "# Umrechnungsfaktoren\n",
    "meter_to_km = 1 / 1000  # 1 Kilometer = 1000 Meter\n",
    "miles_to_km = 1.60934   # 1 Meile = 1.60934 Kilometer\n",
    "\n",
    "# Funktion zur Umrechnung\n",
    "def convert_to_km(value):\n",
    "    if 'km' in value:\n",
    "        return float(value.replace('km', ''))  # Entfernt 'km' und konvertiert direkt\n",
    "    elif 'm' in value:\n",
    "        return float(value.replace('m', '')) * meter_to_km  # Konvertiert Meter in Kilometer\n",
    "    elif 'mi' in value:\n",
    "        return float(value.replace('mi', '')) * miles_to_km  # Konvertiert Meilen in Kilometer\n",
    "    else:\n",
    "        raise ValueError(\"Unbekannte Einheit\")\n",
    "\n",
    "# Transformation anwenden\n",
    "df['Entfernung_km'] = df['Entfernung'].apply(convert_to_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Machmal zusammenfassen in \"Other\"\n",
    "genre_counts = df['Genre'].value_counts()\n",
    "# Visualisation \n",
    "plt.bar(genre_counts.index, genre_counts.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of songs per genre')\n",
    "\n",
    "# nur 4 großen alle anderen = NAN und die werden mit \"Other\" gefüllt\n",
    "df['Genre'] = df['Genre'].map({\n",
    "    'Electronic': 'Electronic',\n",
    "    'Rock': 'Rock',\n",
    "    'Hip-Hop': 'Hip-Hop',\n",
    "    'Pop': 'Pop'\n",
    "}).fillna('Other').astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type of column\n",
    "\n",
    "## is needed for logistic regression not for other \n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "####\n",
    "df['Genre'] = df['Genre'].astype('category')\n",
    "df['Favorite'] = df['Favorite'].astype('bool')\n",
    "\n",
    "df['column'] = df['column'].astype('int64')\n",
    "df['column'] = df['column'].astype('float64')\n",
    "\n",
    "df['column'] = pd.to_datetime(df['column'])  # Datum umwandeln\n",
    "\n",
    "\n",
    "df['column'] = df['column'].astype('string')\n",
    "df['column'] = df['column'].astype('object')\n",
    "## alle types: \n",
    "print(df.dtypes)\n",
    "\n",
    "### Für viele Spalten\n",
    "df[[\"Like\",\"CookTime\",\"PrepTime\",\"RecipeCategory\"]] = df[[\"Like\",\"CookTime\",\"PrepTime\",\"RecipeCategory\"]].astype(float)\n",
    "# Identify non-numeric columns\n",
    "## is needed for logistic regression not for other \n",
    "\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply one-hot encoding to non-numeric columns -## is needed for logistic regression not for other \n",
    "df = pd.get_dummies(df, columns=non_numeric_cols, drop_first=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "private_data = data[data['id'].notna()]  # Rows with entries in 'id' (no label)\n",
    "training_data = data[data['label'].notna()]  # Rows with entries in 'label' (no id)\n",
    "\n",
    "print(private_data)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split for trainign and private set \n",
    "\n",
    "train_df = df[reviews_pd[\"TestSetId\"].isna()]\n",
    "private_df = df[~reviews_pd[\"TestSetId\"].isna()]\n",
    "\n",
    "private_data = data[data['id'].notna()]  # Rows with entries in 'id' (no label)\n",
    "training_data = data[data['label'].notna()]  # Rows with entries in 'label' (no id)\n",
    "\n",
    "print(private_data)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For from sklearn.linear_model \n",
    "# hier default ist mit Intercept \n",
    "X = df[[\"x1\", \"x2\"]].to_numpy()\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "\n",
    "# oder\n",
    "y = data[\"y\"]\n",
    "X = data[[\"x1\", \"x2\"]]\n",
    "# X_with_intercept = sm.add_constant(X)  # Adds a column of ones for the intercept\n",
    "# y = data[\"y\"]\n",
    "\n",
    "## oder wenn alle außer y in Model\n",
    "\n",
    "X = training_data.drop(columns=['label'])\n",
    "y = training_data['label']\n",
    "\n",
    "# X = sm.add_constant(data[[\"x1\", \"x2\", \"x3\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# With numpy arrays for X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# With one pandas dataframe\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-test split - au Tut \n",
    "# stritify stellt sicher das Churn in beiden Sets gleich verteilt ist - machen wenn viele ja und wenige nein zB unfälle \n",
    "class_counts = df['y'].value_counts() # mit normalize=True für Porzentsatz \n",
    "print(class_counts)\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, stratify=df['y'], random_state=2023+2024)\n",
    "\n",
    "# View proportions of Churn\n",
    "print(f\"Proportions Train:\\n {train_df['Churn'].value_counts(normalize=True)}\")\n",
    "print(f\"Proportions Test:\\n {test_df['Churn'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Na = 0 setzen wenn Na eine Bedeutung hat\n",
    "- drop Na wenn nur sehr wenige NAs (eig eher nicht da wir auch alle Vars predicten müssen)\n",
    "- sonst entwedder NA = mean oder Na = most frequent value setzen (je nachdem welche spaltenart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, locate the columns that have at least one missing value (None, NaN, NaT, and similar).\n",
    "print(df.isna().any())  # -> delivery_date, price, tax, date_of_birth\n",
    "\n",
    "# Mean-Wert aus Trainingsdaten berechnen\n",
    "mean_value = training_data['Spalte'].mean()\n",
    "\n",
    "# Fehlende Werte auffüllen\n",
    "training_data['Spalte'] = training_data['Spalte'].fillna(mean_value)\n",
    "private_data['Spalte'] = private_data['Spalte'].fillna(mean_value)\n",
    "\n",
    "df['Spalte'] = df['Spalte'].fillna(df['Spalte'].mean())\n",
    "df['Spalte'] = df['Spalte'].fillna(df['Spalte'].mode()[0])\n",
    "\n",
    "\n",
    "\n",
    "# Drop Na\n",
    "df = df.dropna()\n",
    "# oder\n",
    "df = df.dropna(subset=['x1', 'x3']) # alle Zeilen  mit NA in x1 oder x3 werden gelöscht\n",
    "\n",
    "# Fill NAs\n",
    "df['x2'] = df['x2'].fillna((df['x1'] + df['x3'])/2)\n",
    "## oder\n",
    "df[\"x2\"] = df[\"x2\"].fillna(df[\"x2\"].median())\n",
    "\n",
    "# wenn NA in der Kategorie als Meaningful dann - extra Kategoeire - wenn Grund für Fehlen\n",
    "df['categorical_feature'] = df['categorical_feature'].fillna('Missing')                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scalar ? / Normalisation - nur wichtig für Log. Regression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data\n",
    "X = data.drop(columns=[\"Id\", \"label\"])  # Drop non-feature columns\n",
    "y = data[\"label\"]\n",
    "\n",
    "# Step 1: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Compute mean and std only on X_train\n",
    "\n",
    "# Step 3: Transform training and testing sets using the fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train)  # Transform X_train\n",
    "X_test_scaled = scaler.transform(X_test)    # Transform X_test\n",
    "\n",
    "# Step 4: Transform the private dataset using the same scaler\n",
    "private_features = private_data.drop(columns=[\"Id\", \"label\"], errors=\"ignore\")\n",
    "private_features_scaled = scaler.transform(private_features)  # Transform private data\n",
    "\n",
    "# Now X_train_scaled, X_test_scaled, and private_features_scaled are ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outlier treatment - und wie rausbekommen ob outliers \n",
    "# Check for outliers detailed in other Cheat Sheet\n",
    "\n",
    "# Method 1: Boxplot Visualization\n",
    "sns.boxplot(x=df['feature'])\n",
    "plt.title(\"Boxplot of 'feature'\")\n",
    "plt.show()\n",
    "\n",
    "# Method 2: Z-Score Outliers\n",
    "df['zscore'] = zscore(df['feature'])\n",
    "outliers_z = df[df['zscore'].abs() >= 3]\n",
    "print(f\"Number of outliers detected using Z-Score: {len(outliers_z)}\")\n",
    "\n",
    "# Method 3: IQR Outliers\n",
    "Q1 = df['feature'].quantile(0.25)\n",
    "Q3 = df['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = df[(df['feature'] < lower_bound) | (df['feature'] > upper_bound)]\n",
    "print(f\"Number of outliers detected using IQR: {len(outliers_iqr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transormation agains Outliers\n",
    "\n",
    "df['log_feature'] = np.log1p(df['feature'])  # log1p handles log(0) safely -- checken ob andere damals benutzt\n",
    "df['sqrt_feature'] = np.sqrt(df['feature'])\n",
    "X[\"experience^2\"] = np.square(data[\"experience\"]) # weigthing if many vars around 0 with few outliers\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "df['winsorized_feature'] = winsorize(df['feature'], limits=[0.01, 0.01])\n",
    "# limits=[0.01, 0.01] means bottom 1% and top 1% are capped\n",
    "\n",
    "# Values exceeding a threshold are capped at the threshold.\n",
    "lower_bound, upper_bound = df['feature'].quantile([0.01, 0.99])\n",
    "df['clipped_feature'] = df['feature'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Use the median when data is skewed because it is robust to outliers\n",
    "median = df['feature'].median()\n",
    "df.loc[df['feature'] > upper_bound, 'feature'] = median\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers: use zscore if normally distributed otherweise IQR\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df['zscore'] = zscore(df['feature'])\n",
    "df_no_outliers = df[df['zscore'].abs() < 3]  # Keep rows where z-score < 3\n",
    "print(df_no_outliers)\n",
    "\n",
    "# Remove outliers using IQR\n",
    "Q1 = df['feature'].quantile(0.25)\n",
    "Q3 = df['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_no_outliers = df[(df['feature'] >= lower_bound) & (df['feature'] <= upper_bound)]\n",
    "print(df_no_outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge Data # Beispiel: Schlüsselprüfung\n",
    "print(dataset1['ID'].nunique(), len(dataset1))  # Einzigartigkeit prüfen\n",
    "print(set(dataset1.columns).intersection(set(dataset2.columns)))  # Gemeinsame Spalte\n",
    "# gleicher Typ für schlüsselspalten\n",
    "dataset1['ID'] = dataset1['ID'].astype(str)\n",
    "dataset2['ID'] = dataset2['ID'].astype(str)\n",
    "\n",
    "\n",
    "# Remove duplicate rows based on \"Id\" column\n",
    "dataset1 = dataset1.drop_duplicates(subset='Id', keep='first')\n",
    "dataset2 = dataset2.drop_duplicates(subset='Id', keep='first')\n",
    "# Beispiel für Merging\n",
    "merged_data = pd.merge(dataset1, dataset2, on='ID', how='inner')  # Inner Join\n",
    "\n",
    "merged_data = pd.merge(dataset1, dataset2, on=['Key1', 'Key2'], how='inner')  # Inner Join mit zwei Schlüsseln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left on und right on wenn spalten unterschiedlichen heißen\n",
    "\n",
    "artists_with_behavior = artists.merge(user_behavior, left_on=\"ArtistId\", right_on=\"Artists\")\n",
    "\n",
    "pd.merge(df1, df2, on='key', how='outer')\n",
    "\n",
    "#'inner': Keeps only matching rows (default).\n",
    "#'outer': Keeps all rows from both DataFrames (fills missing data with NaN).\n",
    "#'left': Keeps all rows from the left DataFrame.\n",
    "# 'right': Keeps all rows from the right DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispiel-Datensätze\n",
    "dataset1 = pd.DataFrame({'ID': [1, 2, 3], 'Value1': [10, 20, 30]})\n",
    "dataset2 = pd.DataFrame({'ID': [1, 2, 4], 'Value2': [100, 200, 400]})\n",
    "dataset3 = pd.DataFrame({'ID': [1, 3, 5], 'Value3': [1000, 3000, 5000]})\n",
    "\n",
    "# Iteratives Merging\n",
    "merged_data = pd.merge(dataset1, dataset2, on='ID', how='inner')  # Merge 1 und 2\n",
    "merged_data = pd.merge(merged_data, dataset3, on='ID', how='inner')  # Merge mit 3\n",
    "\n",
    "print(merged_data)\n",
    "## oder\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# Beispiel-Datensätze in einer Liste\n",
    "datasets = [\n",
    "    pd.DataFrame({'ID': [1, 2, 3], 'Value1': [10, 20, 30]}),\n",
    "    pd.DataFrame({'ID': [1, 2, 4], 'Value2': [100, 200, 400]}),\n",
    "    pd.DataFrame({'ID': [1, 3, 5], 'Value3': [1000, 3000, 5000]})\n",
    "]\n",
    "\n",
    "# Reduzierter Merge\n",
    "merged_data = reduce(lambda left, right: pd.merge(left, right, on='ID', how='inner'), datasets)\n",
    "\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generell Check was beste Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models (swap these out as needed)\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=500, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=10),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=10),\n",
    "    'AdaBoost': AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10), n_estimators=50)\n",
    "}\n",
    "\n",
    "# Train, test, and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.2f}\")\n",
    "    print(f\"{name} Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression \n",
    "\n",
    "# Feature Scaling: Logistic regression requires scaled features for optimal performance because it is sensitive to feature magnitudes.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Importance:\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized Data for models that need it (e.g., Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "private_features_scaled = scaler.transform(private_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "logistic_y_pred = logistic_model.predict(X_test_scaled)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_y_pred)\n",
    "private_logistic_predictions = logistic_model.predict(private_features_scaled)\n",
    "print(\"Logistic Regression Accuracy on Test Data:\", logistic_accuracy)\n",
    "print(\"Logistic Regression Predictions on Private Data:\", private_logistic_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization strength (smaller values = stronger regularization)\n",
    "    'penalty': ['l2', 'none'],  # Regularization type\n",
    "    'solver': ['lbfgs', 'liblinear']  # Solvers compatible with the penalties\n",
    "}\n",
    "\n",
    "# Initialize variables for the best model\n",
    "best_model_lr = None\n",
    "best_balanced_accuracy_lr = 0\n",
    "\n",
    "# Standardize features (Logistic Regression requires scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train_resampled_scaled, y_train_resampled):\n",
    "    # Split data into training and validation sets\n",
    "    X_train_train, X_train_val = (\n",
    "        X_train_resampled_scaled[train_index], X_train_resampled_scaled[val_index]\n",
    "    )\n",
    "    y_train_train, y_train_val = (\n",
    "        y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    )\n",
    "\n",
    "    # Iterate over hyperparameter grid\n",
    "    for C in param_grid['C']:\n",
    "        for penalty in param_grid['penalty']:\n",
    "            for solver in param_grid['solver']:\n",
    "                try:\n",
    "                    # Initialize and train Logistic Regression model\n",
    "                    model = LogisticRegression(C=C, penalty=penalty, solver=solver, random_state=42, max_iter=1000)\n",
    "                    model.fit(X_train_train, y_train_train)\n",
    "\n",
    "                    # Predict on validation set\n",
    "                    y_train_val_pred = model.predict(X_train_val)\n",
    "\n",
    "                    # Calculate Balanced Accuracy\n",
    "                    bal_acc = balanced_accuracy_score(y_train_val, y_train_val_pred)\n",
    "\n",
    "                    # Update best model if accuracy improves\n",
    "                    if bal_acc > best_balanced_accuracy_lr:\n",
    "                        best_balanced_accuracy_lr = bal_acc\n",
    "                        best_model_lr = model\n",
    "                except ValueError:\n",
    "                    # Catch incompatible combinations of solver and penalty\n",
    "                    continue\n",
    "\n",
    "# Step 6: Evaluate on test data\n",
    "y_test_pred = best_model_lr.predict(X_test_scaled)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best Logistic Regression Model:\", best_model_lr)\n",
    "print(\"Best Balanced Accuracy (LR):\", best_balanced_accuracy_lr)\n",
    "print(\"Validation Balanced Accuracy of Best Model:\", best_balanced_accuracy_lr)\n",
    "print(\"Test Balanced Accuracy:\", test_balanced_accuracy)\n",
    "\n",
    "# Step 7: Apply the model to new (private) data\n",
    "# Ensure private data is preprocessed like the training data\n",
    "private_features = private_data.drop(columns=[\"Id\", \"label\"], errors=\"ignore\")\n",
    "private_features_scaled = scaler.transform(private_features)\n",
    "\n",
    "# Predict on private data\n",
    "private_predictions = best_model_lr.predict(private_features_scaled)\n",
    "print(\"Predictions on Private Data:\", private_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Model\n",
    "\n",
    "# Define Model\n",
    "train_model = RandomForestClassifier(n_estimators=1000, max_features=3, random_state=0) \n",
    "# n_estimators zwischen 10, 200 und 1000 - schhauen wie lange Trainingszeit dauert \n",
    "# max feautes 3 - kann gut gegen Overfitting sein - muss evtl optimierit werden \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "random_forest_y_pred = random_forest_model.predict(X_test)\n",
    "random_forest_accuracy = accuracy_score(y_test, random_forest_y_pred)\n",
    "private_random_forest_predictions = random_forest_model.predict(private_features)\n",
    "print(\"Random Forest Accuracy on Test Data:\", random_forest_accuracy)\n",
    "print(\"Random Forest Predictions on Private Data:\", private_random_forest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_model_rf = None\n",
    "best_balanced_accuracy_rf = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train_resampled, y_train_resampled):\n",
    "    # Split data into training and validation sets\n",
    "    X_train_train, X_train_val = X_train_resampled.iloc[train_index], X_train_resampled.iloc[val_index]\n",
    "    y_train_train, y_train_val = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    \n",
    "    # Iterate over hyperparameter grid\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for n_estimators in param_grid['n_estimators']:\n",
    "            for min_samples_split in param_grid['min_samples_split']:\n",
    "                # Initialize and train Random Forest model\n",
    "                model = RandomForestClassifier(\n",
    "                    max_depth=max_depth, \n",
    "                    n_estimators=n_estimators, \n",
    "                    min_samples_split=min_samples_split,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train_train, y_train_train)\n",
    "                \n",
    "                # Predict on validation set\n",
    "                y_train_val_pred = model.predict(X_train_val)\n",
    "                \n",
    "                # Calculate Balanced Accuracy\n",
    "                bal_acc = balanced_accuracy_score(y_train_val, y_train_val_pred)\n",
    "                \n",
    "                # Update best model if accuracy improves\n",
    "                if bal_acc > best_balanced_accuracy_rf:\n",
    "                    best_balanced_accuracy_rf = bal_acc\n",
    "                    best_model_rf = model\n",
    "\n",
    "print(\"Best Random Forest Model:\", best_model_rf)\n",
    "print(\"Best Balanced Accuracy (RF):\", best_balanced_accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 6: Evaluate on test data\n",
    "y_test_pred = best_model_rf.predict(X_test)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best Model Hyperparameters:\", best_model_rf.get_params())\n",
    "print(\"Validation Balanced Accuracy of Best Model:\", best_balanced_accuracy_rf)\n",
    "print(\"Test Balanced Accuracy:\", test_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Apply the model to new (private) data\n",
    "\n",
    "# Ensure private data is preprocessed like the training data\n",
    "private_features = private_data.drop(columns=[\"Id\", \"label\"], errors=\"ignore\")\n",
    "\n",
    "# Align private features with training feature set\n",
    "private_features = pd.get_dummies(private_features)\n",
    "private_features = private_features.reindex(columns=X_train_resampled.columns, fill_value=0)\n",
    "\n",
    "# Predict on private data\n",
    "private_predictions = best_model_rf.predict(private_features)\n",
    "\n",
    "print(\"Predictions on Private Data:\", private_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier (tree.DecisionTreeClassifier)\n",
    "# Feature Scaling: Not required. Decision Trees are not sensitive to feature magnitudes.\n",
    "# Overfitting: Decision Trees are prone to overfitting on training data, so you should limit\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=10, min_samples_split=5)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "decision_tree_y_pred = decision_tree_model.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test, decision_tree_y_pred)\n",
    "private_decision_tree_predictions = decision_tree_model.predict(private_features)\n",
    "print(\"Decision Tree Accuracy on Test Data:\", decision_tree_accuracy)\n",
    "print(\"Decision Tree Predictions on Private Data:\", private_decision_tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_model_dt = None\n",
    "best_balanced_accuracy_dt = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train_resampled, y_train_resampled):\n",
    "    # Split data into training and validation sets\n",
    "    X_train_train, X_train_val = X_train_resampled.iloc[train_index], X_train_resampled.iloc[val_index]\n",
    "    y_train_train, y_train_val = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    \n",
    "    # Iterate over hyperparameter grid\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            # Initialize and train Decision Tree model\n",
    "            model = DecisionTreeClassifier(\n",
    "                max_depth=max_depth, \n",
    "                min_samples_split=min_samples_split,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_train_train, y_train_train)\n",
    "            \n",
    "            # Predict on validation set\n",
    "            y_train_val_pred = model.predict(X_train_val)\n",
    "            \n",
    "            # Calculate Balanced Accuracy\n",
    "            bal_acc = balanced_accuracy_score(y_train_val, y_train_val_pred)\n",
    "            \n",
    "            # Update best model if accuracy improves\n",
    "            if bal_acc > best_balanced_accuracy_dt:\n",
    "                best_balanced_accuracy_dt = bal_acc\n",
    "                best_model_dt = model\n",
    "\n",
    "print(\"Best Decision Tree Model:\", best_model_dt)\n",
    "print(\"Best Balanced Accuracy (DT):\", best_balanced_accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 6: Evaluate on test data\n",
    "y_test_pred = best_model_dt.predict(X_test)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best Model Hyperparameters:\", best_model_dt.get_params())\n",
    "print(\"Validation Balanced Accuracy of Best Model:\", best_balanced_accuracy_dt)\n",
    "print(\"Test Balanced Accuracy:\", test_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Apply the model to new (private) data\n",
    "\n",
    "# Ensure private data is preprocessed like the training data\n",
    "private_features = private_data.drop(columns=[\"Id\", \"label\"], errors=\"ignore\")\n",
    "\n",
    "# Align private features with training feature set\n",
    "private_features = pd.get_dummies(private_features)\n",
    "private_features = private_features.reindex(columns=X_train_resampled.columns, fill_value=0)\n",
    "\n",
    "# Predict on private data\n",
    "private_predictions = best_model_dt.predict(private_features)\n",
    "\n",
    "print(\"Predictions on Private Data:\", private_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada Boost - Nicht benutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "# Key parameters are n_estimators (number of weak classifiers) and the learning_rate (shrinkage factor).\n",
    "model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10), n_estimators=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier(random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "adaboost_y_pred = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_y_pred)\n",
    "private_adaboost_predictions = adaboost_model.predict(private_features)\n",
    "print(\"AdaBoost Accuracy on Test Data:\", adaboost_accuracy)\n",
    "print(\"AdaBoost Predictions on Private Data:\", private_adaboost_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of weak learners\n",
    "    'learning_rate': [0.01, 0.1, 1.0],  # Learning rate for boosting\n",
    "    'base_max_depth': [1, 2, 3]  # Maximum depth of the weak learner\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_model_ab = None\n",
    "best_balanced_accuracy_ab = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train_resampled, y_train_resampled):\n",
    "    # Split data into training and validation sets\n",
    "    X_train_train, X_train_val = X_train_resampled.iloc[train_index], X_train_resampled.iloc[val_index]\n",
    "    y_train_train, y_train_val = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    \n",
    "    # Iterate over hyperparameter grid\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            for base_max_depth in param_grid['base_max_depth']:\n",
    "                # Initialize and train AdaBoost model\n",
    "                base_model = DecisionTreeClassifier(max_depth=base_max_depth, random_state=42)\n",
    "                model = AdaBoostClassifier(\n",
    "                    estimator=base_model,\n",
    "                    n_estimators=n_estimators,\n",
    "                    learning_rate=learning_rate,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train_train, y_train_train)\n",
    "                \n",
    "                # Predict on validation set\n",
    "                y_train_val_pred = model.predict(X_train_val)\n",
    "                \n",
    "                # Calculate Balanced Accuracy\n",
    "                bal_acc = balanced_accuracy_score(y_train_val, y_train_val_pred)\n",
    "                \n",
    "                # Update best model if accuracy improves\n",
    "                if bal_acc > best_balanced_accuracy_ab:\n",
    "                    best_balanced_accuracy_ab = bal_acc\n",
    "                    best_model_ab = model\n",
    "\n",
    "print(\"Best AdaBoost Model:\", best_model_ab)\n",
    "print(\"Best Balanced Accuracy (AdaBoost):\", best_balanced_accuracy_ab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predictions for private data to CSV\n",
    "private_data_predictions = pd.DataFrame({\n",
    "    'Id': private_data['Id'],  # Assuming 'Id' exists in private_data\n",
    "    'Predicted_Label': private_adaboost_predictions  # Change to desired model's predictions\n",
    "})\n",
    "private_data_predictions.to_csv(\"private_data_predictions.csv\", index=False)\n",
    "print(\"Private data predictions saved to 'private_data_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation - works for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross-validation - works for all models\n",
    "cv_fits_accuracy = cross_val_score(train_model, X, y, cv=4, scoring='balanced_accuracy')\n",
    "cv_fits_accuracy = cross_val_score(train_model, X, y, cv=4, scoring='accuracy')\n",
    "cv_fits_precision = cross_val_score(train_model, X, y, cv=4, scoring='precision')\n",
    "cv_fits_recall = cross_val_score(train_model, X, y, cv=4, scoring='recall')\n",
    "\n",
    "print(\"\\nCV-Accuracy:\", np.mean(cv_fits_accuracy))\n",
    "print(\"CV-Precision:\", np.mean(cv_fits_precision))\n",
    "print(\"CV-Recall:\", np.mean(cv_fits_recall))\n",
    "#Recall ist wichtig, wenn Falsch-Negative vermieden werden müssen.\n",
    "# Precision ist wichtig, wenn Falsch-Positive vermieden werden müssen.\n",
    "\n",
    "# Train the final model\n",
    "train_model.fit(train_df.drop(columns=['Churn']), train_df['Churn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variable Importance Plot\n",
    "importance_values = train_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importance_values})\n",
    "imp_plot = importance_df.plot(kind='bar', x='Feature', y='Importance', legend=False)\n",
    "imp_plot.plot()\n",
    "plt.show()\n",
    "\n",
    "# Apply on test set\n",
    "test_predictions = train_model.predict(test_df.drop(columns=['Churn']))\n",
    "test_probabilities = train_model.predict_proba(test_df.drop(columns=['Churn']))\n",
    "\n",
    "test_predictions_df = pd.DataFrame({'Churn': test_df['Churn'], \n",
    "                                     'Predicted_Churn': test_predictions,\n",
    "                                     'Probability_Churn=0': test_probabilities[:, 0],\n",
    "                                     'Probability_Churn=1': test_probabilities[:, 1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion Matrix - same for all models\n",
    "conf_matrix = confusion_matrix(test_df['Churn'], test_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "## if Importance bei 0 oder 1% und Correlation über 0.8 - droppen?\n",
    "# wenn sehr viele NAs evtl droppen außer sehr wichtig\n",
    "\n",
    "\n",
    "# Precision, accuracy, recall\n",
    "print(\"\\nTest-Precision:\", precision_score(test_df['Churn'], test_predictions))\n",
    "print(\"Test-Accuracy:\", accuracy_score(test_df['Churn'], test_predictions))\n",
    "print(\"Test-Recall:\", recall_score(test_df['Churn'], test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Random Forest Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning of Hyperparameters - auch für alle Models\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_features': [2, 3, 4, 'sqrt'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "## oder\n",
    "# Dann wenn sehr viele Parameter lieber das unten nehmen \n",
    "\n",
    "#####\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Definiere das Modell\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Definiere die Verteilung der Parameter für RandomizedSearch\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),    # Zufällige Zahl zwischen 100 und 1000\n",
    "    'max_features': ['sqrt', 'log2', None], # Auswahl der Features pro Baum\n",
    "    'max_depth': [None, 10, 20, 30],        # Maximale Tiefe des Baums\n",
    "    'min_samples_split': randint(2, 10),    # Minimum an Samples pro Split\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV mit 5-facher Kreuzvalidierung\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=5, random_state=0)\n",
    "\n",
    "# Trainiere das Modell\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Beste Parameter und beste Leistung\n",
    "best_params_random = random_search.best_params_\n",
    "best_score_random = random_search.best_score_\n",
    "\n",
    "print(\"Beste Parameter:\", best_params_random)\n",
    "print(\"Beste Kreuzvalidierungsgenauigkeit:\", best_score_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selction:\n",
    "# only relecant for decisoin trees und random forest\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "\n",
    "# Step 1: Feature Selection\n",
    "selected_features = ['x1', 'x2', 'x3']  # Features selected based on importance\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of X_text and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Use probabilities for ROC AUC\n",
    "\n",
    "## wie prediction für private set\n",
    "# Step 5: Predict on the private data set\n",
    "private_predictions = model.predict(private_features)\n",
    "output = private_data[['id']].copy()\n",
    "output['prediction'] = private_predictions\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Export the data set to CSV\n",
    "output.to_csv(\"output_predictions.csv\", index=False)\n",
    "# Final Output\n",
    "print(\"Output predictions saved to 'output_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access Data / if needeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wert für zweispalten herausfinden mit auch mit loc möglich - difference is index and other is labeling - anschauen im internet \n",
    "df[\"Visitors (million)\"][df[\"Year\"]==1995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming columns\n",
    "EUR = 1/1.95583\n",
    "print(EUR)\n",
    "df.loc[df['Year'] <= 2001, 'Min price'] *= EUR\n",
    "\n",
    "# during periods\n",
    "print(df_merge.loc[(df_merge['Year'] >= 2000) & (df_merge['Year'] <= 2007), 'Min price'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the best RF model\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_model_rf = None\n",
    "best_balanced_accuracy_rf = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train_resampled, y_train_resampled):\n",
    "    # Split data into training and validation sets\n",
    "    X_train_train, X_train_val = X_train_resampled.iloc[train_index], X_train_resampled.iloc[val_index]\n",
    "    y_train_train, y_train_val = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    \n",
    "    # Iterate over hyperparameter grid\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for n_estimators in param_grid['n_estimators']:\n",
    "            for min_samples_split in param_grid['min_samples_split']:\n",
    "                # Initialize and train Random Forest model\n",
    "                model = RandomForestClassifier(\n",
    "                    max_depth=max_depth, \n",
    "                    n_estimators=n_estimators, \n",
    "                    min_samples_split=min_samples_split,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train_train, y_train_train)\n",
    "                \n",
    "                # Predict on validation set\n",
    "                y_train_val_pred = model.predict(X_train_val)\n",
    "                \n",
    "                # Calculate Balanced Accuracy\n",
    "                bal_acc = balanced_accuracy_score(y_train_val, y_train_val_pred)\n",
    "                \n",
    "                # Update best model if accuracy improves\n",
    "                if bal_acc > best_balanced_accuracy_rf:\n",
    "                    best_balanced_accuracy_rf = bal_acc\n",
    "                    best_model_rf = model\n",
    "\n",
    "print(\"Best Random Forest Model:\", best_model_rf)\n",
    "print(\"Best Balanced Accuracy (RF):\", best_balanced_accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train the best model on the entire training set\n",
    "best_model_rf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Evaluate on test data\n",
    "y_test_pred = best_model_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best Model Hyperparameters:\", best_model.get_params())\n",
    "print(\"Validation Accuracy of Best Model:\", best_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESAMPLING of Data - start erstmal mit stratisfaction - sollte passen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampliong with duplicates\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "X_minority = X_train[y_train == 0]\n",
    "X_majority = X_train[y_train == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "X_upsampled, y_upsampled = resample(\n",
    "    X_minority,\n",
    "    y_train[y_train == 0],\n",
    "    replace=True,  # With replacement\n",
    "    n_samples=len(X_majority),  # Match majority class size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "X_balanced = np.concatenate((X_majority, X_upsampled))\n",
    "y_balanced = np.concatenate((y_train[y_train == 1], y_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SMOTE (sythetic) - wsl eher nicht benutzen \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Perform SMOTE upsampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_upsampled, y_upsampled = smote.fit_resample(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample majority class\n",
    "X_downsampled, y_downsampled = resample(\n",
    "    X_train[y_train == 1],\n",
    "    y_train[y_train == 1],\n",
    "    replace=False,  # Without replacement\n",
    "    n_samples=np.sum(y_train == 0),  # Match minority class size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "X_balanced = np.concatenate((X_downsampled, X_train[y_train == 0]))\n",
    "y_balanced = np.concatenate((y_downsampled, y_train[y_train == 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
